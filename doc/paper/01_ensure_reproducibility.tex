% Build on the outline below.  Cite by using \citep{Author2016} to add a
% parenthetical citation; use \citet{Author2016} to get a textual cite like
% this: Author (2016). 

\section{Ensure Reproducibility}

What is reproducibility? Reproducibility means that Researcher B obtains exactly the same results that were originally reported by Researcher A (e.g. the author of that paper) from A’s data when following the same methodology \citep{Brunswik1955; Asendorpf2013}. Even though it seems like an unimportant process, certainly it is not. Reproducibility is ``the gold standard for scientific research'' \citep[1]{Janz2015} because replicating existing studies is ``the only way to understand and evaluate an empirical analysis fully'' \citep[444]{King1995}. This is why the American Political Science Association (APSA) implemented new guidelines for replication that requires researchers to provide enough information about how they collected the data they used and how they used data to arrive at the published results \citep{Lupia2014}.

To be specific, ensuring reproducibility can help our discipline in multiple ways. First, without reproducibility, it is impossible to determine the strength or robustness of published results \citep{Lupia2014}. As the LaCour Scandal in 2015 revealed, the advanced method, the rigorous experimentation process, and the authority of the famous academic journal themselves do not guarantee the reliability of findings. Only after a couple of researchers in the same field attempted to replicate LaCour and Green’s findings and failed to do so, our discipline could find out that their findings are completely fraud \citep{Young2015}. As \citet[62]{Dafoe2014} argues, ``Fragile, misleading, and nonreplicable statistical analyses can be largely eliminated'' by the replication process. 
Second, reproducibility ensures that the future researchers can enjoy ``all the benefits of the first researcher’s hard work'' \citep[445]{King1995}.  Political science has developed as the collective works of numerous researchers. As we can see in the term ``community enterprise'', researchers can develop their ideas and methods by extending high-quality existing research \citep{King1995}. However, without enough information about the existing research, the process of collective works cannot continue. Compared to other disciplines, political science requires researchers to spend more time for deciding how to measure and quantify real world observations and events. In this sense, when scholars fail to document enough information about how they collected and used data for results, the published article cannot be fully understandable and effectively interpretable by other scholars \citep{Lupia2010}.

However, in spite of the shared consensus about the importance of reproducibility and the announced APSA guidelines on data access and research transparency, reproducibility is still a difficult mission to complete. Scholars often do not want to share their data because they have invested a great deal of time and other resources into collecting data and want to get benefits from their data as many as possible \citep{Lupia2014}. As the bare minimum for replication, now researchers share their replication data in repositories such as the Inter-university Consortium for Political and Social Research (ICPSR), the Dataverse Network at Harvard University, and other journal-specific archives, but we find out that even the replication data in these repositories cannot guarantee the reproducibility of their findings. Newman, Johnston, and Lown's (\citeyear{Newman2015a}) replication materials about ``False Consciousness or Class Awareness? Local Income Inequality, Personal Economic Position, and Belief in American Meritocracy'' do not lead us to obtain the same results as the original paper. They shared just simple codes for table results without the detailed information about how their data is collected and used. Furthermore, this replication paper finds out that the results of Table 1, 2, and 3 of the original paper are not reproducible even with their codes and data shared in the Dataverse Network. In their replication R code, they admit that ``the coefficients (of this replication file) will therefore be slightly different, but the signs, significance, and effect sizes remain the same.'' If we cannot get the same results with this data, how can we call this data as the ``replication data''? As we will show below, the results of Table 1 and 2 are not reproducible exactly, and the result of Table 3 is not reproducible at all because there are more parameters than observations. 


Reproducibility as bare minimum for replication; DA-RT APSA guidelines

script all work

packrat and checkpoint packages in R; version command in Stata

quote \citet{Newman2015} replication materials

Table 1 and 2 cannot be reproduced exactly

Table 3 cannot be reproduced at all: more parameters than observations

% See also, maybe, http://andrewgelman.com/2015/09/14/its-not-so-easy-to-share-data-and-code-and-there-are-lots-of-bureaucrats-who-spend-their-time-making-it-even-more-difficult/
